{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da40d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import base64\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b58fd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# Connection credentials\n",
    "REED_API_KEY = \"ff3b8ce7-3225-4336-911f-3b1ecad843ec\"\n",
    "\n",
    "# Azure PostgreSQL\n",
    "DB_HOST = \"dvdrental.postgres.database.azure.com\"\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"gbadmin\"\n",
    "DB_PASSWORD = \"Catherin3!\"\n",
    "DB_PORT = 5432\n",
    "\n",
    "# Reed API authentication\n",
    "auth_string = f\"{REED_API_KEY}:\"\n",
    "encoded_auth = base64.b64encode(auth_string.encode()).decode()\n",
    "reed_headers = {'Authorization': f'Basic {encoded_auth}'}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e61b9e",
   "metadata": {},
   "source": [
    "# Daily Job Update - Reed API to Azure PostgreSQL\n",
    "\n",
    "This notebook fetches 100 new Data Analyst jobs from Reed.co.uk API daily and adds them to the Azure PostgreSQL database, avoiding duplicates.\n",
    "\n",
    "**How it works:**\n",
    "1. Connect to Azure PostgreSQL and get existing job IDs\n",
    "2. Fetch 100+ jobs from Reed API\n",
    "3. Filter out jobs already in the database\n",
    "4. Extract skills from new jobs\n",
    "5. Insert only new jobs (up to 100) into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04a1f6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Connecting to Azure PostgreSQL...\n",
      "‚úÖ Connected to Azure PostgreSQL\n",
      "üìä Found 99 existing jobs in database\n",
      "‚úÖ Connected to Azure PostgreSQL\n",
      "üìä Found 99 existing jobs in database\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Connect to Azure PostgreSQL and get existing job IDs\n",
    "print(\"üîÑ Connecting to Azure PostgreSQL...\")\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        database=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        port=DB_PORT,\n",
    "        sslmode='require',\n",
    "        connect_timeout=10\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    print(\"‚úÖ Connected to Azure PostgreSQL\")\n",
    "    \n",
    "    # Get all existing job IDs to avoid duplicates\n",
    "    cursor.execute(\"SELECT jobid FROM jobs;\")\n",
    "    existing_job_ids = set(row[0] for row in cursor.fetchall())\n",
    "    print(f\"üìä Found {len(existing_job_ids)} existing jobs in database\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection error: {e}\")\n",
    "    existing_job_ids = set()\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98375365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Fetching jobs from Reed API...\n",
      "üì• Fetched 200 jobs from API\n",
      "   (Total available: 2558)\n",
      "‚ú® Found 194 NEW jobs (not in database)\n",
      "üìå Will add 100 new jobs to database\n",
      "üì• Fetched 200 jobs from API\n",
      "   (Total available: 2558)\n",
      "‚ú® Found 194 NEW jobs (not in database)\n",
      "üìå Will add 100 new jobs to database\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Search for recent Data Analyst jobs from Reed API\n",
    "print(\"üîÑ Fetching jobs from Reed API...\")\n",
    "\n",
    "def search_reed_jobs(keywords='data analyst', results_to_take=100, skip=0):\n",
    "    \"\"\"Search Reed.co.uk for jobs\"\"\"\n",
    "    search_url = 'https://www.reed.co.uk/api/1.0/search'\n",
    "    params = {\n",
    "        'keywords': keywords,\n",
    "        'resultsToTake': results_to_take,\n",
    "        'resultsToSkip': skip\n",
    "    }\n",
    "    \n",
    "    response = requests.get(search_url, headers=reed_headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['results'], data['totalResults']\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return [], 0\n",
    "\n",
    "# Fetch jobs - get more than 100 to ensure we have enough new ones\n",
    "jobs_batch_1, total = search_reed_jobs('data analyst', results_to_take=100, skip=0)\n",
    "jobs_batch_2, _ = search_reed_jobs('data analyst', results_to_take=100, skip=100)\n",
    "\n",
    "all_searched_jobs = jobs_batch_1 + jobs_batch_2\n",
    "print(f\"üì• Fetched {len(all_searched_jobs)} jobs from API\")\n",
    "print(f\"   (Total available: {total})\")\n",
    "\n",
    "# Filter out jobs already in database\n",
    "new_jobs = [job for job in all_searched_jobs if job['jobId'] not in existing_job_ids]\n",
    "print(f\"‚ú® Found {len(new_jobs)} NEW jobs (not in database)\")\n",
    "\n",
    "# Take only the first 100 new jobs\n",
    "new_jobs_to_add = new_jobs[:100]\n",
    "print(f\"üìå Will add {len(new_jobs_to_add)} new jobs to database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cf8db1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Fetching full job details...\n",
      "  Progress: 0/100...\n",
      "  Progress: 10/100...\n",
      "  Progress: 10/100...\n",
      "  Progress: 20/100...\n",
      "  Progress: 20/100...\n",
      "  Progress: 30/100...\n",
      "  Progress: 30/100...\n",
      "  Progress: 40/100...\n",
      "  Progress: 40/100...\n",
      "  Progress: 50/100...\n",
      "  Progress: 50/100...\n",
      "  Progress: 60/100...\n",
      "  Progress: 60/100...\n",
      "  Progress: 70/100...\n",
      "  Progress: 70/100...\n",
      "  Progress: 80/100...\n",
      "  Progress: 80/100...\n",
      "  Progress: 90/100...\n",
      "  Progress: 90/100...\n",
      "‚úÖ Fetched full details for 100 jobs\n",
      "‚úÖ Fetched full details for 100 jobs\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Fetch FULL details for new jobs\n",
    "print(\"üîÑ Fetching full job details...\")\n",
    "\n",
    "def get_job_details(job_id):\n",
    "    \"\"\"Get full details for a specific job\"\"\"\n",
    "    url = f'https://www.reed.co.uk/api/1.0/jobs/{job_id}'\n",
    "    response = requests.get(url, headers=reed_headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error fetching job {job_id}: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "new_jobs_full = []\n",
    "for i, job in enumerate(new_jobs_to_add):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"  Progress: {i}/{len(new_jobs_to_add)}...\")\n",
    "    \n",
    "    details = get_job_details(job['jobId'])\n",
    "    if details:\n",
    "        new_jobs_full.append(details)\n",
    "    \n",
    "    time.sleep(0.1)  # Be nice to the API\n",
    "\n",
    "print(f\"‚úÖ Fetched full details for {len(new_jobs_full)} jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b275e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Extracting skills from job descriptions...\n",
      "‚úÖ Extracted skills from 100 jobs\n",
      "\n",
      "Sample of new jobs:\n",
      "      jobId                 jobTitle                     employerName  \\\n",
      "0  56107225             Data Analyst                        NG Bailey   \n",
      "1  56107954             Data Analyst                      esure Group   \n",
      "2  55988131             Data Analyst                  Smart4Chemicals   \n",
      "3  56094030             Data Analyst  Marley Risk Consultants Limited   \n",
      "4  56038552  Data Analyst Apprentice                               QA   \n",
      "\n",
      "     locationName  \n",
      "0           Leeds  \n",
      "1         Reigate  \n",
      "2           Wigan  \n",
      "3  Bishops Cleeve  \n",
      "4       Edgbaston  \n",
      "‚úÖ Extracted skills from 100 jobs\n",
      "\n",
      "Sample of new jobs:\n",
      "      jobId                 jobTitle                     employerName  \\\n",
      "0  56107225             Data Analyst                        NG Bailey   \n",
      "1  56107954             Data Analyst                      esure Group   \n",
      "2  55988131             Data Analyst                  Smart4Chemicals   \n",
      "3  56094030             Data Analyst  Marley Risk Consultants Limited   \n",
      "4  56038552  Data Analyst Apprentice                               QA   \n",
      "\n",
      "     locationName  \n",
      "0           Leeds  \n",
      "1         Reigate  \n",
      "2           Wigan  \n",
      "3  Bishops Cleeve  \n",
      "4       Edgbaston  \n"
     ]
    }
   ],
   "source": [
    "# Step 4: Extract skills from job descriptions\n",
    "print(\"üîÑ Extracting skills from job descriptions...\")\n",
    "\n",
    "# Skills mapping (same as in scraping.ipynb)\n",
    "skills_mapping = {\n",
    "    'Python': ['python', 'py'],\n",
    "    'R': [r'\\br\\b', r'\\br programming\\b'],\n",
    "    'SQL': ['sql', 'mysql', 'postgresql', 'postgres', 't-sql', 'pl/sql', 'mssql'],\n",
    "    'SAS': ['sas'],\n",
    "    'VBA': ['vba', 'visual basic'],\n",
    "    'Tableau': ['tableau'],\n",
    "    'Power BI': ['power bi', 'powerbi', 'power-bi'],\n",
    "    'Looker': ['looker'],\n",
    "    'Qlik': ['qlik', 'qliksense', 'qlikview'],\n",
    "    'Excel': ['excel', 'advanced excel', 'ms excel', 'spreadsheet'],\n",
    "    'AWS': ['aws', 'amazon web services'],\n",
    "    'Azure': ['azure', 'microsoft azure'],\n",
    "    'GCP': ['gcp', 'google cloud'],\n",
    "    'MongoDB': ['mongodb', 'mongo'],\n",
    "    'Oracle': ['oracle database', 'oracle db'],\n",
    "    'SQL Server': ['sql server', 'mssql', 'microsoft sql'],\n",
    "    'Snowflake': ['snowflake'],\n",
    "    'Pandas': ['pandas'],\n",
    "    'NumPy': ['numpy'],\n",
    "    'Spark': ['spark', 'pyspark', 'apache spark'],\n",
    "    'ETL': ['etl'],\n",
    "    'Airflow': ['airflow', 'apache airflow'],\n",
    "    'Statistics': ['statistics', 'statistical analysis', 'statistical modeling'],\n",
    "    'Machine Learning': ['machine learning', 'ml'],\n",
    "    'A/B Testing': ['a/b test', 'ab test'],\n",
    "    'Git': ['git', 'github', 'gitlab'],\n",
    "    'Jira': ['jira'],\n",
    "    'API': ['api', 'rest api', 'restful'],\n",
    "}\n",
    "\n",
    "def extract_skills_improved(text):\n",
    "    \"\"\"Extract skills from job description\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Clean HTML tags\n",
    "    if '<' in text and '>' in text:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "    \n",
    "    found_skills = set()\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for skill_name, patterns in skills_mapping.items():\n",
    "        for pattern in patterns:\n",
    "            regex = r'\\b' + pattern.replace(' ', r'\\s+') + r'\\b'\n",
    "            if re.search(regex, text_lower, re.IGNORECASE):\n",
    "                found_skills.add(skill_name)\n",
    "                break\n",
    "    \n",
    "    return list(found_skills)\n",
    "\n",
    "# Create DataFrame and extract skills\n",
    "df_new_jobs = pd.DataFrame(new_jobs_full)\n",
    "df_new_jobs['skills'] = df_new_jobs['jobDescription'].apply(extract_skills_improved)\n",
    "df_new_jobs['skills_list'] = df_new_jobs['skills'].apply(lambda x: ', '.join(x) if len(x) > 0 else '')\n",
    "\n",
    "print(f\"‚úÖ Extracted skills from {len(df_new_jobs)} jobs\")\n",
    "print(f\"\\nSample of new jobs:\")\n",
    "print(df_new_jobs[['jobId', 'jobTitle', 'employerName', 'locationName']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49c7e249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Preparing data for database insertion...\n",
      "   Dropped columns: ['minimumsalary', 'maximumsalary', 'currency', 'salarytype', 'salary', 'externalurl', 'parttime', 'skills']\n",
      "‚úÖ Data cleaned and transformed\n",
      "   Rows: 100\n",
      "   Columns: ['employerid', 'employername', 'jobid', 'jobtitle', 'locationname', 'yearlyminimumsalary', 'yearlymaximumsalary', 'dateposted', 'expirationdate', 'joburl', 'fulltime', 'contracttype', 'jobdescription', 'applicationcount', 'skills_list', 'daysremaining']\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Clean and transform data (match existing table structure)\n",
    "print(\"üîÑ Preparing data for database insertion...\")\n",
    "\n",
    "def clean_new_jobs(df):\n",
    "    \"\"\"Apply same transformations as in column_changes_to_jobs.ipynb\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Lowercase all column names FIRST\n",
    "    df_clean.columns = df_clean.columns.str.lower()\n",
    "    \n",
    "    # Drop columns not in the database\n",
    "    columns_to_remove = [\n",
    "        \"minimumsalary\", \"maximumsalary\", \"currency\", \"salarytype\", \n",
    "        \"salary\", \"externalurl\", \"parttime\", \"skills\"\n",
    "    ]\n",
    "    existing_cols_to_remove = [col for col in columns_to_remove if col in df_clean.columns]\n",
    "    if existing_cols_to_remove:\n",
    "        df_clean = df_clean.drop(columns=existing_cols_to_remove)\n",
    "        print(f\"   Dropped columns: {existing_cols_to_remove}\")\n",
    "    \n",
    "    # Convert expirationdate and add daysremaining\n",
    "    if 'expirationdate' in df_clean.columns:\n",
    "        df_clean['expirationdate'] = pd.to_datetime(df_clean['expirationdate'], format='%d/%m/%Y', errors='coerce')\n",
    "        df_clean['daysremaining'] = (df_clean['expirationdate'] - datetime.now()).dt.days\n",
    "        df_clean['expirationdate'] = df_clean['expirationdate'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Convert fulltime to text\n",
    "    if 'fulltime' in df_clean.columns:\n",
    "        df_clean['fulltime'] = df_clean['fulltime'].replace({True: 'Full time', False: 'Part time'})\n",
    "    \n",
    "    # Clean HTML from job descriptions and remove invalid Unicode\n",
    "    def clean_text(text):\n",
    "        if pd.notna(text):\n",
    "            # Remove HTML tags\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            clean = soup.get_text()\n",
    "            # Remove surrogate pairs and other problematic Unicode characters\n",
    "            clean = clean.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "            return clean\n",
    "        return text\n",
    "    \n",
    "    df_clean['jobdescription'] = df_clean['jobdescription'].apply(clean_text)\n",
    "    \n",
    "    # Also clean other text fields that might have Unicode issues\n",
    "    text_columns = ['jobtitle', 'employername', 'locationname', 'contracttype']\n",
    "    for col in text_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].apply(\n",
    "                lambda x: x.encode('utf-8', errors='ignore').decode('utf-8') if pd.notna(x) else x\n",
    "            )\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_new_jobs_clean = clean_new_jobs(df_new_jobs)\n",
    "print(f\"‚úÖ Data cleaned and transformed\")\n",
    "print(f\"   Rows: {len(df_new_jobs_clean)}\")\n",
    "print(f\"   Columns: {list(df_new_jobs_clean.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7cd7ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Inserting new jobs into database...\n",
      "‚úÖ Successfully inserted 100 new jobs into database!\n",
      "üìä Total jobs in database: 200\n",
      "‚úÖ Successfully inserted 100 new jobs into database!\n",
      "üìä Total jobs in database: 200\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Insert new jobs into Azure PostgreSQL database\n",
    "if conn and not df_new_jobs_clean.empty:\n",
    "    try:\n",
    "        print(\"üîÑ Inserting new jobs into database...\")\n",
    "        \n",
    "        # Use SQLAlchemy for easier insertion\n",
    "        password_encoded = urllib.parse.quote_plus(DB_PASSWORD)\n",
    "        connection_string = f\"postgresql://{DB_USER}:{password_encoded}@{DB_HOST}:{DB_PORT}/{DB_NAME}?sslmode=require\"\n",
    "        engine = create_engine(connection_string, connect_args={'connect_timeout': 30})\n",
    "        \n",
    "        # Insert new jobs (append to existing table)\n",
    "        df_new_jobs_clean.to_sql('jobs', engine, if_exists='append', index=False, method='multi', chunksize=100)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully inserted {len(df_new_jobs_clean)} new jobs into database!\")\n",
    "        \n",
    "        # Verify total count\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM jobs;\")\n",
    "        total_jobs = cursor.fetchone()[0]\n",
    "        print(f\"üìä Total jobs in database: {total_jobs}\")\n",
    "        \n",
    "        engine.dispose()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inserting into database: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    if not conn:\n",
    "        print(\"‚ö†Ô∏è  No database connection\")\n",
    "    if df_new_jobs_clean.empty:\n",
    "        print(\"‚ö†Ô∏è  No new jobs to insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d29fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Backup saved to new_jobs_backup_20251127_172642.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Save backup to CSV\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "csv_filename = f\"new_jobs_backup_{timestamp}.csv\"\n",
    "df_new_jobs_clean.to_csv(csv_filename, index=False)\n",
    "print(f\"üíæ Backup saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "413d7d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection closed\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "‚úÖ Found 194 new jobs from Reed API\n",
      "‚úÖ Inserted 100 jobs into database\n",
      "‚úÖ Backup saved to new_jobs_backup_20251127_172642.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Close database connection\n",
    "if conn:\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"‚úÖ Database connection closed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Found {len(new_jobs)} new jobs from Reed API\")\n",
    "print(f\"‚úÖ Inserted {len(df_new_jobs_clean)} jobs into database\")\n",
    "print(f\"‚úÖ Backup saved to {csv_filename}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
